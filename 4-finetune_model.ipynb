{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010b1638-4b2a-4006-8fdf-5e74fa5bd062",
   "metadata": {},
   "source": [
    "# Fine-tuning gemma 3 for Tool Use\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffefcad-3b12-4a39-8ae7-1b09a8f385f9",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee22cf-d1dc-4737-b05c-0fa638f5acbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95ddcd-9a43-420c-a296-f48e90a98bde",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454c256-331f-4850-9842-a1733ed702cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'query_type', 'trace', 'num_tools_available', 'tool_needed', 'tool_name'],\n",
       "        num_rows: 477\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['query', 'query_type', 'trace', 'num_tools_available', 'tool_needed', 'tool_name'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query', 'query_type', 'trace', 'num_tools_available', 'tool_needed', 'tool_name'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "ds = load_dataset(\"shawhin/tool-use-finetuning\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f2366-4018-47e5-b0eb-a1b232f461ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4186604db23429a9d78b6c695c5a588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364a61b25b95442db9bc19e4320ea213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf95f3b1c304c9e977b29d142326954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def filter_dataset(example):\n",
    "    # Keep all \"easy\" queries that need tools\n",
    "    if example['query_type'] == 'easy' and example['tool_needed'] == True:\n",
    "        return True\n",
    "    \n",
    "    # Keep 20% of \"no_tool\" queries\n",
    "    if example['query_type'] == 'no_tool':\n",
    "        return np.random.random() < 0.2\n",
    "    \n",
    "    # Exclude everything else\n",
    "    return False\n",
    "\n",
    "# Apply the filtering\n",
    "ds = ds.filter(filter_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78725b-85ca-48d4-8208-dc97bc354f6b",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50aa9d2-5a95-4931-a917-095423218532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(f\"MPS is available: {mps_device}\")\n",
    "else:\n",
    "    print(\"MPS is not available. Please check your macOS version, PyTorch installation, and hardware.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad8969-21a9-4fe8-8df9-841fa62ff2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"mps\",\n",
    "    attn_implementation='eager'\n",
    ")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afed4c-6ec8-4bda-9dee-2e728138418f",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d159be-a913-459a-b5b9-768666a08c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(row):\n",
    "    # replace first user message role to system\n",
    "    messages = row['trace']\n",
    "    messages[0]['role'] = 'system'\n",
    "\n",
    "    # add tokenized text to dataset\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False, return_tensors=\"pt\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c43410-d593-4c8d-a5ba-5e78fb23bc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da76105732da499a883453f0c18c99b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc3729cad804cd49150f629a87f403d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb868aef0ac450b83c617d1deec0bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effff96-0d1c-4652-9aa1-bf7ac237d38a",
   "metadata": {},
   "source": [
    "### define LoRA hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70e9d7-34d3-4814-88ee-f5be40e32710",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "target_modules = \"all-linear\"\n",
    "\n",
    "peft_config = LoraConfig(r=r,\n",
    "                         lora_alpha=lora_alpha,\n",
    "                         lora_dropout=lora_dropout,\n",
    "                         target_modules=target_modules,\n",
    "                         bias=\"none\",\n",
    "                         task_type=TaskType.CAUSAL_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7bf9a-2a37-4875-bf0d-49bfba3ee3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,045,760 || all params: 1,012,931,712 || trainable%: 1.2879\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17fe01d-946f-4e2b-8f44-2c274caacdbe",
   "metadata": {},
   "source": [
    "### define training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25db0a-7d86-4e1b-a0e6-6ca407da1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 2e-4\n",
    "num_epochs = 3\n",
    "batch_size = 1\n",
    "finetuned_model_name = \"gemma-3-1b-tool-use\"\n",
    "\n",
    "# define training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=f\"models/{finetuned_model_name}\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_ratio = 0.03,\n",
    "    max_grad_norm = 0.3,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    eval_steps=20,\n",
    "    save_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=False,\n",
    "    fp16=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de6ed1-71a6-47de-938f-4caa0d4008c7",
   "metadata": {},
   "source": [
    "### fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362b95b-45d3-4c20-9320-09610315d8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76840b0430b64fbb98a00e0713676091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1393b98986a14baa9647a817d19367f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7c69bb5e02485da9933d225c5d4a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee05f8e23674e76b05790e333e4eb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db4d24fa36444889599b7f8499cd428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545f44355496479cb4a096043b31fe6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/Users/shaw/Documents/_code/_stv/sandbox/tool-use-ft/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 09:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.713200</td>\n",
       "      <td>0.223053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.150600</td>\n",
       "      <td>0.135271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.097300</td>\n",
       "      <td>0.122439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaw/Documents/_code/_stv/sandbox/tool-use-ft/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/shaw/Documents/_code/_stv/sandbox/tool-use-ft/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 24s, sys: 2min 32s, total: 5min 57s\n",
      "Wall time: 9min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=0.26859559933344523, metrics={'train_runtime': 596.8328, 'train_samples_per_second': 0.985, 'train_steps_per_second': 0.126, 'total_flos': 2295453414240000.0, 'train_loss': 0.26859559933344523})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be4c78e-ba25-47c3-9205-46f1e3438a75",
   "metadata": {},
   "source": [
    "### push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a9fde-7a58-47e9-8aa2-33bd0ee20c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1af206e2734ab48f0842dc0bf592ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/6.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb987d9f6924cc696b2e135ac6297bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/52.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3f034c58314162a3eacbca96801ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shawhin/gemma-3-1b-tool-use/commit/c05fb405b2a0985f86f9467f09d2725a91a7d109', commit_message='shawhin/gemma-3-1b-tool-use', commit_description='', oid='c05fb405b2a0985f86f9467f09d2725a91a7d109', pr_url=None, repo_url=RepoUrl('https://huggingface.co/shawhin/gemma-3-1b-tool-use', endpoint='https://huggingface.co', repo_type='model', repo_id='shawhin/gemma-3-1b-tool-use'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push to hub\n",
    "username = \"charbull\"\n",
    "trainer.push_to_hub(f\"{username}/{finetuned_model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
