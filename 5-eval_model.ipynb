{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f773a076-2a57-4096-8dba-71231f8f25e3",
   "metadata": {},
   "source": [
    "# Evaluate Fine-tuned Model\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42de5c-6819-47c0-9c71-5df88ba09fa7",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2126e55-02d1-4397-9741-41b11f4c060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "from utils.tool_calling import parse_tool_call, call_tool\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc86ff2-f1aa-4c7a-b0ad-a3c32c07bacd",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbbb9055-d960-40a1-ab4d-cfc429f4028f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36f6cf8af7144eca6b26e1d76dc880b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb85905a75d2494e90b4c359b86b21a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/800k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906fbf3fa90b4c3e84d4cfef2806d744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/95.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f753d9086a443f393b7238ba87a21f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/105k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6422c04748426994786c3719e8a85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9efed070a814547977a4a3bebd04d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd80d1a5906c41d7a0c3c17755414152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "ds = load_dataset(\"shawhin/tool-use-finetuning\")\n",
    "ds_test = ds['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41344cac-d573-4cb1-8d0a-074acd93189f",
   "metadata": {},
   "source": [
    "### load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f306fed0-3b4d-4829-9ab2-5c16db874a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"mps\",\n",
    ")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1658c1f0-67c7-4034-865d-e4e0ddec266c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e684373f69f34afc9de92d5eb8c83da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/903 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53cfef512384ad3b585504e1ce563d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/52.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load fine-tuned model\n",
    "finetuned_model_name = \"shawhin/gemma-3-1b-tool-use\"\n",
    "\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    finetuned_model_name,\n",
    "    device_map=\"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f744c-9d0d-4d11-89eb-1cfd8bef404c",
   "metadata": {},
   "source": [
    "### generate responses using models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f19c67-84da-4bc3-bde7-7d480df6914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_tool_calling(generator, row):\n",
    "    \"\"\"\n",
    "    Evaluates whether the model correctly identifies the need for a tool,\n",
    "    calls the correct tool, and executes it successfully.\n",
    "    \"\"\"\n",
    "    # Extract values from row\n",
    "    messages = row['trace']\n",
    "    expected_tool_name = row.get('tool_name', None)\n",
    "\n",
    "    # Auto-infer if a tool is needed\n",
    "    tool_needed = expected_tool_name is not None and str(expected_tool_name).lower() != 'nan'\n",
    "\n",
    "    # Set system role on first message\n",
    "    messages = messages.copy()\n",
    "    messages[0]['role'] = 'system'\n",
    "\n",
    "    # Generate assistant output\n",
    "    output = generator(messages[:2], return_full_text=True)[0]\n",
    "    response = output['generated_text'][-1]\n",
    "\n",
    "    # Initialize tracking flags\n",
    "    model_called_tool = False\n",
    "    model_tool_name = None\n",
    "    model_called_correct_tool = False\n",
    "    tool_call_success = False\n",
    "\n",
    "    # Check if model issued a tool call\n",
    "    if \"<tool_call>\" in response['content']:\n",
    "        model_called_tool = True\n",
    "        parsed_result = parse_tool_call(response['content'])\n",
    "\n",
    "        if parsed_result is not None:\n",
    "            tool_name, tool_args = parsed_result\n",
    "            model_tool_name = tool_name\n",
    "\n",
    "            model_called_correct_tool = (tool_name == expected_tool_name)\n",
    "\n",
    "            try:\n",
    "                result = call_tool(tool_name, tool_args)\n",
    "                tool_call_success = True\n",
    "            except:\n",
    "                tool_call_success = False\n",
    "        else:\n",
    "            # tool_call marker present but malformed\n",
    "            model_tool_name = None\n",
    "            model_called_correct_tool = False\n",
    "            tool_call_success = False\n",
    "\n",
    "    # Final evaluation: did the model call a tool when it was needed?\n",
    "    if tool_needed:\n",
    "        model_called_tool_when_needed = model_called_tool\n",
    "    else:\n",
    "        model_called_tool_when_needed = not model_called_tool\n",
    "\n",
    "    return {\n",
    "        'response': response,\n",
    "        'model_called_tool': model_called_tool,\n",
    "        'model_tool_name': model_tool_name,\n",
    "        'model_called_tool_when_needed': model_called_tool_when_needed,\n",
    "        'model_called_correct_tool': model_called_correct_tool,\n",
    "        'tool_call_success': tool_call_success,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0cd6b5-d712-4d10-9709-e5b91cff61ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "# create pipelines\n",
    "base_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, temperature=0.1)\n",
    "finetuned_generator = pipeline(\"text-generation\", model=finetuned_model, tokenizer=tokenizer, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829878cd-2512-42e3-8671-c4387d34320c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating row: 0\n",
      "Evaluating row: 1\n",
      "Evaluating row: 2\n",
      "Evaluating row: 3\n",
      "Evaluating row: 4\n",
      "Evaluating row: 5\n",
      "Evaluating row: 6\n",
      "Evaluating row: 7\n",
      "Evaluating row: 8\n",
      "Evaluating row: 9\n",
      "Evaluating row: 10\n",
      "Evaluating row: 11\n",
      "Evaluating row: 12\n",
      "Evaluating row: 13\n",
      "Evaluating row: 14\n",
      "Evaluating row: 15\n",
      "Evaluating row: 16\n",
      "Evaluating row: 17\n",
      "Evaluating row: 18\n",
      "Evaluating row: 19\n",
      "Evaluating row: 20\n",
      "Evaluating row: 21\n",
      "Evaluating row: 22\n",
      "Evaluating row: 23\n",
      "Evaluating row: 24\n",
      "Evaluating row: 25\n",
      "Evaluating row: 26\n",
      "Evaluating row: 27\n",
      "Evaluating row: 28\n",
      "Evaluating row: 29\n",
      "Evaluating row: 30\n",
      "Evaluating row: 31\n",
      "Evaluating row: 32\n",
      "Evaluating row: 33\n",
      "Evaluating row: 34\n",
      "Evaluating row: 35\n",
      "Evaluating row: 36\n",
      "Evaluating row: 37\n",
      "Evaluating row: 38\n",
      "Evaluating row: 39\n",
      "Evaluating row: 40\n",
      "Evaluating row: 41\n",
      "Evaluating row: 42\n",
      "Evaluating row: 43\n",
      "Evaluating row: 44\n",
      "Evaluating row: 45\n",
      "Evaluating row: 46\n",
      "Evaluating row: 47\n",
      "Evaluating row: 48\n",
      "Evaluating row: 49\n",
      "Evaluating row: 50\n",
      "Evaluating row: 51\n",
      "Evaluating row: 52\n",
      "Evaluating row: 53\n",
      "Evaluating row: 54\n",
      "Evaluating row: 55\n",
      "Evaluating row: 56\n",
      "Evaluating row: 57\n",
      "Evaluating row: 58\n",
      "Evaluating row: 59\n",
      "CPU times: user 18min 7s, sys: 2min 10s, total: 20min 18s\n",
      "Wall time: 21min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_data = []\n",
    "\n",
    "for i, row in enumerate(ds_test):\n",
    "    print(\"Evaluating row:\", i)\n",
    "    # Generate base model results\n",
    "    base_results = evaluate_model_tool_calling(base_generator, row)\n",
    "    result_row = {\n",
    "        'model_name': model_name,\n",
    "        'query': row['query'],\n",
    "        'query_type': row['query_type'],\n",
    "        'num_tools_available': row['num_tools_available'],\n",
    "        'expected_tool_name': row['tool_name'],\n",
    "        **base_results\n",
    "    }\n",
    "    results_data.append(result_row)\n",
    "\n",
    "    # Generate fine-tuned model results\n",
    "    finetuned_results = evaluate_model_tool_calling(finetuned_generator, row)\n",
    "    finetuned_result_row = {\n",
    "        'model_name': finetuned_model_name,\n",
    "        'query': row['query'],\n",
    "        'query_type': row['query_type'],\n",
    "        'num_tools_available': row['num_tools_available'],\n",
    "        'expected_tool_name': row['tool_name'],\n",
    "        **finetuned_results\n",
    "    }\n",
    "    results_data.append(finetuned_result_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "064e35e0-cfcb-4692-915e-7d98d3ccedb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5883f552-15c5-492f-9b65-cffcdc143f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to file\n",
    "results_df.to_csv('data/eval_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc08109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>query</th>\n",
       "      <th>query_type</th>\n",
       "      <th>num_tools_available</th>\n",
       "      <th>expected_tool_name</th>\n",
       "      <th>response</th>\n",
       "      <th>model_called_tool</th>\n",
       "      <th>model_tool_name</th>\n",
       "      <th>model_called_tool_when_needed</th>\n",
       "      <th>model_called_correct_tool</th>\n",
       "      <th>tool_call_success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google/gemma-3-1b-it</td>\n",
       "      <td>Hy... can u pls tell me how many days is it be...</td>\n",
       "      <td>hard</td>\n",
       "      <td>23</td>\n",
       "      <td>calculate_date_diff</td>\n",
       "      <td>{'role': 'assistant', 'content': '```xml\n",
       "&lt;tool...</td>\n",
       "      <td>True</td>\n",
       "      <td>calculate_date_diff</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shawhin/gemma-3-1b-tool-use</td>\n",
       "      <td>Hy... can u pls tell me how many days is it be...</td>\n",
       "      <td>hard</td>\n",
       "      <td>23</td>\n",
       "      <td>calculate_date_diff</td>\n",
       "      <td>{'role': 'assistant', 'content': '&lt;tool_call&gt;\n",
       "...</td>\n",
       "      <td>True</td>\n",
       "      <td>calculate_date_diff</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google/gemma-3-1b-it</td>\n",
       "      <td>What are some affordable ways to discover new ...</td>\n",
       "      <td>no_tool</td>\n",
       "      <td>37</td>\n",
       "      <td>None</td>\n",
       "      <td>{'role': 'assistant', 'content': '```xml\n",
       "&lt;fina...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shawhin/gemma-3-1b-tool-use</td>\n",
       "      <td>What are some affordable ways to discover new ...</td>\n",
       "      <td>no_tool</td>\n",
       "      <td>37</td>\n",
       "      <td>None</td>\n",
       "      <td>{'role': 'assistant', 'content': '&lt;final_answe...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>google/gemma-3-1b-it</td>\n",
       "      <td>What is the current date and time right now?</td>\n",
       "      <td>easy</td>\n",
       "      <td>20</td>\n",
       "      <td>get_current_datetime</td>\n",
       "      <td>{'role': 'assistant', 'content': '```xml\n",
       "&lt;tool...</td>\n",
       "      <td>True</td>\n",
       "      <td>get_current_datetime</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model_name  \\\n",
       "0         google/gemma-3-1b-it   \n",
       "1  shawhin/gemma-3-1b-tool-use   \n",
       "2         google/gemma-3-1b-it   \n",
       "3  shawhin/gemma-3-1b-tool-use   \n",
       "4         google/gemma-3-1b-it   \n",
       "\n",
       "                                               query query_type  \\\n",
       "0  Hy... can u pls tell me how many days is it be...       hard   \n",
       "1  Hy... can u pls tell me how many days is it be...       hard   \n",
       "2  What are some affordable ways to discover new ...    no_tool   \n",
       "3  What are some affordable ways to discover new ...    no_tool   \n",
       "4       What is the current date and time right now?       easy   \n",
       "\n",
       "   num_tools_available    expected_tool_name  \\\n",
       "0                   23   calculate_date_diff   \n",
       "1                   23   calculate_date_diff   \n",
       "2                   37                  None   \n",
       "3                   37                  None   \n",
       "4                   20  get_current_datetime   \n",
       "\n",
       "                                            response  model_called_tool  \\\n",
       "0  {'role': 'assistant', 'content': '```xml\n",
       "<tool...               True   \n",
       "1  {'role': 'assistant', 'content': '<tool_call>\n",
       "...               True   \n",
       "2  {'role': 'assistant', 'content': '```xml\n",
       "<fina...              False   \n",
       "3  {'role': 'assistant', 'content': '<final_answe...              False   \n",
       "4  {'role': 'assistant', 'content': '```xml\n",
       "<tool...               True   \n",
       "\n",
       "        model_tool_name  model_called_tool_when_needed  \\\n",
       "0   calculate_date_diff                           True   \n",
       "1   calculate_date_diff                           True   \n",
       "2                  None                           True   \n",
       "3                  None                           True   \n",
       "4  get_current_datetime                           True   \n",
       "\n",
       "   model_called_correct_tool  tool_call_success  \n",
       "0                       True              False  \n",
       "1                       True               True  \n",
       "2                      False              False  \n",
       "3                      False              False  \n",
       "4                       True               True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a139c46-7f22-4267-9d72-6e0d1b9176f2",
   "metadata": {},
   "source": [
    "### evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769d0eb-542d-449e-9879-d4de49406f01",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_model_performance(results_df):\n",
    "    \"\"\"\n",
    "    Compare model performance based on tool calling metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    results_df (pd.DataFrame): DataFrame containing evaluation results with columns:\n",
    "        - model_name\n",
    "        - model_called_tool_when_needed\n",
    "        - model_called_correct_tool\n",
    "        - tool_call_success\n",
    "        - expected_tool_name\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Performance comparison with models as rows and metrics as columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter for rows where a tool call was needed (expected_tool_name is not None/NaN)\n",
    "    tool_needed_rows = results_df[results_df['expected_tool_name'].notna() & \n",
    "                                 (results_df['expected_tool_name'] != 'nan')]\n",
    "    \n",
    "    # Group by model name and calculate pass rates\n",
    "    performance_metrics = results_df.groupby('model_name').agg({\n",
    "        'model_called_tool_when_needed': 'mean',\n",
    "    }).round(4)\n",
    "    \n",
    "    # Calculate metrics only for rows where tool was needed\n",
    "    if len(tool_needed_rows) > 0:\n",
    "        tool_metrics = tool_needed_rows.groupby('model_name').agg({\n",
    "            'model_called_correct_tool': 'mean',\n",
    "            'tool_call_success': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Combine the metrics\n",
    "        performance_metrics = pd.concat([performance_metrics, tool_metrics], axis=1)\n",
    "    else:\n",
    "        # If no rows need tools, set these metrics to None\n",
    "        performance_metrics['model_called_correct_tool'] = None\n",
    "        performance_metrics['tool_call_success'] = None\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    performance_metrics.columns = [\n",
    "        'Tool Called When Needed (%)',\n",
    "        'Correct Tool Called (%)', \n",
    "        'Tool Call Success (%)'\n",
    "    ]\n",
    "    \n",
    "    # Convert to percentages\n",
    "    performance_metrics = performance_metrics * 100\n",
    "    \n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14a65787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_performance_confusion_matrix(results_df):\n",
    "    \"\"\"\n",
    "    Compare model performance based on tool calling metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    results_df (pd.DataFrame): DataFrame containing evaluation results with columns:\n",
    "        - model_name\n",
    "        - model_called_tool_when_needed_tp\n",
    "        - model_called_tool_when_needed_tn\n",
    "        - model_called_tool_when_needed_fn\n",
    "        - model_called_tool_when_needed_fp\n",
    "        - model_called_correct_tool\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Performance comparison with models as rows and metrics as columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter for rows where a tool call was needed (expected_tool_name is not None/NaN)\n",
    "    tool_needed_rows = results_df[results_df['expected_tool_name'].notna() & \n",
    "                                 (results_df['expected_tool_name'] != 'nan')]\n",
    "\n",
    "    # Calculate TP, TN, FP, FN for tool needed\n",
    "    tool_needed_rows['tp'] = (tool_needed_rows['model_tool_name'] == tool_needed_rows['expected_tool_name']).astype(int)\n",
    "    tool_needed_rows['fn'] = ((tool_needed_rows['model_tool_name'].isna()) & \n",
    "                              (tool_needed_rows['expected_tool_name'].notna())).astype(int) \n",
    "    tool_needed_rows['tn'] = (tool_needed_rows['model_tool_name'].isna() & tool_needed_rows['expected_tool_name'].isna()).astype(int)\n",
    "    tool_needed_rows['fp'] = ((tool_needed_rows['model_tool_name'].notna()) & \n",
    "                              (tool_needed_rows['expected_tool_name'].isna())).astype(int)\n",
    "\n",
    "    # Calculate metrics only for rows where tool was needed\n",
    "    confusion_metrics = tool_needed_rows.groupby('model_name').agg({\n",
    "        'tp': 'sum',\n",
    "        'tn': 'sum',\n",
    "        'fp': 'sum',\n",
    "        'fn': 'sum'\n",
    "    })\n",
    "    confusion_metrics.columns = [\n",
    "        'True Positives',\n",
    "        'True Negatives', \n",
    "        'False Positives',\n",
    "        'False Negatives'\n",
    "    ]\n",
    "    return confusion_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d63383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def model_tool_classification_performance_manual(results_df):\n",
    "    \"\"\"\n",
    "    Calculates per-class and aggregated TP, TN, FP, FN and derived metrics\n",
    "    for each model, without using sklearn.\n",
    "\n",
    "    Parameters:\n",
    "    results_df (pd.DataFrame): DataFrame containing evaluation results with columns:\n",
    "        - model_name\n",
    "        - expected_tool_name (str or None/NaN)\n",
    "        - model_tool_name (str or None/NaN)\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are model names. Each value is a DataFrame\n",
    "          indexed by class name (plus 'MICRO_AVG', 'MACRO_AVG'), with columns\n",
    "          for TP, TN, FP, FN, Precision, Recall, and F1.\n",
    "    \"\"\"\n",
    "    df = results_df.copy()\n",
    "\n",
    "    # Standardize missing values to 'NO_TOOL' string\n",
    "    df['expected_tool_name'] = df['expected_tool_name'].fillna('NO_TOOL')\n",
    "    df['model_tool_name'] = df['model_tool_name'].fillna('NO_TOOL')\n",
    "\n",
    "    all_labels = sorted(list(set(df['expected_tool_name']) | set(df['model_tool_name'])))\n",
    "    model_performance = {}\n",
    "\n",
    "    for model in df['model_name'].unique():\n",
    "        model_df = df[df['model_name'] == model]\n",
    "        y_true = model_df['expected_tool_name'].values\n",
    "        y_pred = model_df['model_tool_name'].values\n",
    "        n_samples = len(y_true)\n",
    "\n",
    "        if n_samples == 0:\n",
    "            continue\n",
    "\n",
    "        per_class_metrics = []\n",
    "        for label in all_labels:\n",
    "            is_true_label = (y_true == label)\n",
    "            is_pred_label = (y_pred == label)\n",
    "\n",
    "            tp = np.sum(is_true_label & is_pred_label)\n",
    "            fp = np.sum(~is_true_label & is_pred_label)\n",
    "            fn = np.sum(is_true_label & ~is_pred_label)\n",
    "            tn = np.sum(~is_true_label & ~is_pred_label)\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "            per_class_metrics.append({\n",
    "                'Class': label,\n",
    "                'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn,\n",
    "                'Precision': precision, 'Recall': recall, 'F1': f1\n",
    "            })\n",
    "\n",
    "        metrics_df = pd.DataFrame(per_class_metrics).set_index('Class')\n",
    "\n",
    "        # --- Aggregated Metrics ---\n",
    "        # Micro-Averaged Metrics (Reflects Overall Accuracy)\n",
    "        correct_predictions = np.sum(y_true == y_pred)\n",
    "        accuracy = correct_predictions / n_samples\n",
    "        micro_avg_row = {\n",
    "            'TP': correct_predictions,\n",
    "            'TN': np.nan,  # TN is not well-defined for micro average in this way\n",
    "            'FP': n_samples - correct_predictions,\n",
    "            'FN': n_samples - correct_predictions,\n",
    "            'Precision': accuracy,\n",
    "            'Recall': accuracy,\n",
    "            'F1': accuracy\n",
    "        }\n",
    "\n",
    "        # Macro-Averaged Metrics\n",
    "        macro_precision = metrics_df['Precision'].mean()\n",
    "        macro_recall = metrics_df['Recall'].mean()\n",
    "        macro_f1 = metrics_df['F1'].mean()\n",
    "        macro_avg_row = {\n",
    "            'TP': np.nan, 'TN': np.nan, 'FP': np.nan, 'FN': np.nan,\n",
    "            'Precision': macro_precision,\n",
    "            'Recall': macro_recall,\n",
    "            'F1': macro_f1\n",
    "        }\n",
    "\n",
    "        # Append aggregated rows\n",
    "        agg_df = pd.DataFrame([micro_avg_row, macro_avg_row], index=['MICRO_AVG', 'MACRO_AVG'])\n",
    "        metrics_df = pd.concat([metrics_df, agg_df])\n",
    "\n",
    "        model_performance[model] = metrics_df\n",
    "\n",
    "    return model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f34cf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'google/gemma-3-1b-it':                           TP    TN    FP    FN  Precision    Recall        F1\n",
       " NO_TOOL                 17.0  37.0   3.0   3.0   0.850000  0.850000  0.850000\n",
       " Wikipedia                0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " add_to_todo_list         1.0  58.0   1.0   0.0   0.500000  1.000000  0.666667\n",
       " calculate_average        0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " calculate_date_diff      1.0  59.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " calculate_min_max        0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " calculate_std_dev        0.0  60.0   0.0   0.0   0.000000  0.000000  0.000000\n",
       " calculate_tax            0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " calculator               2.0  58.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " char_count               1.0  59.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " compute_median           1.0  58.0   1.0   0.0   0.500000  1.000000  0.666667\n",
       " compute_min_max          0.0  58.0   1.0   1.0   0.000000  0.000000  0.000000\n",
       " compute_std_dev          1.0  58.0   0.0   1.0   1.000000  0.500000  0.666667\n",
       " compute_unix_timestamp   0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " convert_currency         1.0  56.0   1.0   2.0   0.500000  0.333333  0.400000\n",
       " convert_time_zone        1.0  58.0   0.0   1.0   1.000000  0.500000  0.666667\n",
       " create_tweet             0.0  60.0   0.0   0.0   0.000000  0.000000  0.000000\n",
       " deduplicate_list         1.0  59.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " extract_hashtags         0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " extract_keywords         0.0  60.0   0.0   0.0   0.000000  0.000000  0.000000\n",
       " extract_phone_number     1.0  59.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " extract_urls             0.0  58.0   2.0   0.0   0.000000  0.000000  0.000000\n",
       " fetch_crypto_price       0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " fetch_stock_price        0.0  57.0   1.0   2.0   0.000000  0.000000  0.000000\n",
       " filter_list              0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " generate_password        0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " get_crypto_price         0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " get_current_datetime     1.0  56.0   2.0   1.0   0.333333  0.500000  0.400000\n",
       " get_news                 0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " get_stock_price          0.0  60.0   0.0   0.0   0.000000  0.000000  0.000000\n",
       " get_unix_timestamp       1.0  58.0   0.0   1.0   1.000000  0.500000  0.666667\n",
       " get_weekday_from_date    0.0  58.0   0.0   2.0   0.000000  0.000000  0.000000\n",
       " notion_search            0.0  57.0   2.0   1.0   0.000000  0.000000  0.000000\n",
       " post_to_x                0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " query_database           0.0  57.0   2.0   1.0   0.000000  0.000000  0.000000\n",
       " retrieve_data            0.0  58.0   1.0   1.0   0.000000  0.000000  0.000000\n",
       " reverse_list             0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " send_email               0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " send_slack_message       0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " set_reminder             1.0  58.0   1.0   0.0   0.500000  1.000000  0.666667\n",
       " slugify_text             1.0  58.0   1.0   0.0   0.500000  1.000000  0.666667\n",
       " sort_list                0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " unit_conversion          0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " web_search               0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " word_count               0.0  58.0   0.0   2.0   0.000000  0.000000  0.000000\n",
       " MICRO_AVG               32.0   NaN  28.0  28.0   0.533333  0.533333  0.533333\n",
       " MACRO_AVG                NaN   NaN   NaN   NaN   0.259630  0.270741  0.251481,\n",
       " 'shawhin/gemma-3-1b-tool-use':                           TP    TN    FP    FN  Precision    Recall        F1\n",
       " NO_TOOL                 20.0  36.0   4.0   0.0   0.833333  1.000000  0.909091\n",
       " Wikipedia                0.0  60.0   0.0   0.0   0.000000  0.000000  0.000000\n",
       " add_to_todo_list         1.0  58.0   1.0   0.0   0.500000  1.000000  0.666667\n",
       " calculate_average        0.0  60.0   0.0   0.0   0.000000  0.000000  0.000000\n",
       " calculate_date_diff      1.0  59.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " calculate_min_max        0.0  60.0   0.0   0.0   0.000000  0.000000  0.000000\n",
       " calculate_std_dev        0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " calculate_tax            0.0  60.0   0.0   0.0   0.000000  0.000000  0.000000\n",
       " calculator               2.0  58.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " char_count               0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " compute_median           1.0  59.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " compute_min_max          0.0  58.0   1.0   1.0   0.000000  0.000000  0.000000\n",
       " compute_std_dev          1.0  58.0   0.0   1.0   1.000000  0.500000  0.666667\n",
       " compute_unix_timestamp   0.0  60.0   0.0   0.0   0.000000  0.000000  0.000000\n",
       " convert_currency         3.0  57.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " convert_time_zone        1.0  58.0   0.0   1.0   1.000000  0.500000  0.666667\n",
       " create_tweet             0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " deduplicate_list         0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " extract_hashtags         0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " extract_keywords         0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " extract_phone_number     1.0  59.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " extract_urls             0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " fetch_crypto_price       0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " fetch_stock_price        0.0  58.0   0.0   2.0   0.000000  0.000000  0.000000\n",
       " filter_list              1.0  58.0   1.0   0.0   0.500000  1.000000  0.666667\n",
       " generate_password        0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " get_crypto_price         0.0  60.0   0.0   0.0   0.000000  0.000000  0.000000\n",
       " get_current_datetime     2.0  56.0   2.0   0.0   0.500000  1.000000  0.666667\n",
       " get_news                 0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " get_stock_price          0.0  58.0   2.0   0.0   0.000000  0.000000  0.000000\n",
       " get_unix_timestamp       2.0  58.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " get_weekday_from_date    2.0  58.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " notion_search            0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " post_to_x                0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " query_database           0.0  58.0   1.0   1.0   0.000000  0.000000  0.000000\n",
       " retrieve_data            0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " reverse_list             1.0  59.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " send_email               0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " send_slack_message       0.0  59.0   1.0   0.0   0.000000  0.000000  0.000000\n",
       " set_reminder             0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " slugify_text             1.0  59.0   0.0   0.0   1.000000  1.000000  1.000000\n",
       " sort_list                0.0  60.0   0.0   0.0   0.000000  0.000000  0.000000\n",
       " unit_conversion          0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " web_search               0.0  59.0   0.0   1.0   0.000000  0.000000  0.000000\n",
       " word_count               1.0  57.0   1.0   1.0   0.500000  0.500000  0.500000\n",
       " MICRO_AVG               41.0   NaN  19.0  19.0   0.683333  0.683333  0.683333\n",
       " MACRO_AVG                NaN   NaN   NaN   NaN   0.307407  0.322222  0.305387}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_summary = model_tool_classification_performance_manual(results_df)\n",
    "eval_summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97f80483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def model_tool_detection_counts(results_df):\n",
    "    \"\"\"\n",
    "    Calculates the confusion matrix components (TP, TN, FP, FN) for each model\n",
    "    based on the binary task of detecting WHETHER a tool call is needed.\n",
    "    This does NOT consider if the correct tool was called, only IF a tool was called.\n",
    "\n",
    "    Positive Class: Tool call is expected.\n",
    "    Negative Class: No tool call is expected.\n",
    "\n",
    "    TP: Expected a tool, Model called any tool.\n",
    "    TN: Expected NO tool, Model called NO tool.\n",
    "    FP: Expected NO tool, Model called any tool.\n",
    "    FN: Expected a tool, Model called NO tool.\n",
    "\n",
    "    Parameters:\n",
    "    results_df (pd.DataFrame): DataFrame containing evaluation results with columns:\n",
    "        - model_name\n",
    "        - expected_tool_name (str or None/NaN)\n",
    "        - model_tool_name (str or None/NaN)\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame indexed by model_name, with columns for TP, TN, FP, FN\n",
    "                  for the task of tool detection.\n",
    "    \"\"\"\n",
    "    df = results_df.copy()\n",
    "\n",
    "    # Handle None/NaN\n",
    "    df['expected_tool_name'] = df['expected_tool_name'].fillna('NO_TOOL')\n",
    "    df['model_tool_name'] = df['model_tool_name'].fillna('NO_TOOL')\n",
    "\n",
    "    # Binary flags for whether a tool was expected or called\n",
    "    expected_positive = (df['expected_tool_name'] != 'NO_TOOL')\n",
    "    predicted_positive = (df['model_tool_name'] != 'NO_TOOL')\n",
    "\n",
    "    # Calculate TP, TN, FP, FN for Tool Detection\n",
    "    df['TP'] = (expected_positive & predicted_positive).astype(int)\n",
    "    df['FN'] = (expected_positive & ~predicted_positive).astype(int)\n",
    "    df['FP'] = (~expected_positive & predicted_positive).astype(int)\n",
    "    df['TN'] = (~expected_positive & ~predicted_positive).astype(int)\n",
    "\n",
    "    # Aggregate results per model\n",
    "    metrics = df.groupby('model_name').agg(\n",
    "        TP=('TP', 'sum'),\n",
    "        TN=('TN', 'sum'),\n",
    "        FP=('FP', 'sum'),\n",
    "        FN=('FN', 'sum')\n",
    "    ) # .reset_index().set_index('model_name')\n",
    "\n",
    "    # Calculate derived metrics\n",
    "    tp = metrics['TP']\n",
    "    tn = metrics['TN']\n",
    "    fp = metrics['FP']\n",
    "    fn = metrics['FN']\n",
    "    total = tp + tn + fp + fn\n",
    "\n",
    "    metrics['Accuracy'] = (tp + tn) / total\n",
    "    metrics['Precision'] = tp / (tp + fp)\n",
    "    metrics['Recall'] = tp / (tp + fn)\n",
    "    metrics['F1'] = 2 * (metrics['Precision'] * metrics['Recall']) / (metrics['Precision'] + metrics['Recall'])\n",
    "\n",
    "    # Handle division by zero cases by filling NaN with 0\n",
    "    metrics = metrics.fillna(0)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf479a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>google/gemma-3-1b-it</th>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shawhin/gemma-3-1b-tool-use</th>\n",
       "      <td>36</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             TP  TN  FP  FN  Accuracy  Precision  Recall  \\\n",
       "model_name                                                                 \n",
       "google/gemma-3-1b-it         37  17   3   3  0.900000      0.925   0.925   \n",
       "shawhin/gemma-3-1b-tool-use  36  20   0   4  0.933333      1.000   0.900   \n",
       "\n",
       "                                   F1  \n",
       "model_name                             \n",
       "google/gemma-3-1b-it         0.925000  \n",
       "shawhin/gemma-3-1b-tool-use  0.947368  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_summary = model_tool_detection_counts(results_df)\n",
    "eval_summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d093785f",
   "metadata": {},
   "source": [
    "Okay, let's break down these results for the two models, `google/gemma-3-1b-it` and `shawhin/gemma-3-1b-tool-use`, based on the \"Tool Detection\" task (i.e., how well they predict *whether* any tool needs to be called).\n",
    "\n",
    "**Total Samples:** For each model, the total number of samples is TP + TN + FP + FN.\n",
    "*   `google/gemma-3-1b-it`: 37 + 17 + 3 + 3 = 60 samples\n",
    "*   `shawhin/gemma-3-1b-tool-use`: 36 + 20 + 0 + 4 = 60 samples\n",
    "\n",
    "**Understanding the Metrics:**\n",
    "\n",
    "*   **TP (True Positives):** Cases where the model correctly predicted a tool was needed.\n",
    "*   **TN (True Negatives):** Cases where the model correctly predicted NO tool was needed.\n",
    "*   **FP (False Positives):** Cases where the model incorrectly predicted a tool was needed (Type I error).\n",
    "*   **FN (False Negatives):** Cases where the model incorrectly predicted NO tool was needed (Type II error).\n",
    "*   **Accuracy:** Overall percentage of correct predictions (both tool needed and not needed). (TP+TN)/Total\n",
    "*   **Precision:** Of all the times the model said \"a tool is needed\", what proportion was correct? TP/(TP+FP)\n",
    "*   **Recall:** Of all the times a tool *was actually* needed, what proportion did the model catch? TP/(TP+FN)\n",
    "*   **F1-Score:** Harmonic mean of Precision and Recall, balancing both.\n",
    "\n",
    "**Analysis of `google/gemma-3-1b-it`:**\n",
    "\n",
    "*   **TP: 37:** Correctly identified the need for a tool 37 times.\n",
    "*   **TN: 17:** Correctly identified that no tool was needed 17 times.\n",
    "*   **FP: 3:** Incorrectly called a tool when none was expected 3 times. This means the model sometimes over-triggers.\n",
    "*   **FN: 3:** Missed the need for a tool 3 times, saying no tool was needed when one was.\n",
    "*   **Accuracy: 0.900 (90%)**: Generally good overall performance.\n",
    "*   **Precision: 0.925 (92.5%)**: When it decided to call a tool, it was correct 92.5% of the time (37 out of 40 times).\n",
    "*   **Recall: 0.925 (92.5%)**: It successfully found 92.5% of all instances where a tool was actually required (37 out of 40 instances).\n",
    "*   **F1-Score: 0.925**: Balanced performance between Precision and Recall.\n",
    "\n",
    "**Analysis of `shawhin/gemma-3-1b-tool-use`:**\n",
    "\n",
    "*   **TP: 36:** Correctly identified the need for a tool 36 times.\n",
    "*   **TN: 20:** Correctly identified that no tool was needed 20 times. Excellent specificity.\n",
    "*   **FP: 0:** *Never* called a tool when none was expected. This model is very conservative about triggering tools, only doing so when confident.\n",
    "*   **FN: 4:** Missed the need for a tool 4 times. This is slightly more than the other model.\n",
    "*   **Accuracy: 0.933 (93.3%)**: Higher overall accuracy than the Google model.\n",
    "*   **Precision: 1.000 (100%)**: Perfect precision! Every time this model decided to call a tool, it was correct in needing one. This is due to having zero False Positives.\n",
    "*   **Recall: 0.900 (90%)**: It found 90% of the instances where a tool was needed (36 out of 40 instances). This is slightly lower than the Google model's recall.\n",
    "*   **F1-Score: 0.947**: Very high F1, indicating excellent overall performance in tool detection, slightly better than the Google model, driven by perfect precision.\n",
    "\n",
    "**Comparison and Key Differences:**\n",
    "\n",
    "*   **Overall Accuracy:** `shawhin/gemma-3-1b-tool-use` is slightly more accurate overall.\n",
    "*   **False Positives:** The most striking difference is the FP rate. The `shawhin` model has 0 FPs, meaning it never cries wolf. The `google` model has 3 FPs, so it sometimes calls tools unnecessarily.\n",
    "*   **False Negatives:** The `shawhin` model has slightly more FNs (4 vs. 3), meaning it's slightly more likely to fail to call a tool when one is needed compared to the `google` model.\n",
    "*   **Precision vs. Recall Trade-off:**\n",
    "    *   `shawhin/gemma-3-1b-tool-use` maximizes **Precision** (at 100%) at the cost of slightly lower Recall. It's cautious and only acts when certain.\n",
    "    *   `google/gemma-3-1b-it` has a more balanced Precision and Recall, both at 92.5%.\n",
    "\n",
    "**Which model is better?**\n",
    "\n",
    "It depends on the cost of errors:\n",
    "\n",
    "*   If **False Positives are costly** (e.g., unnecessary API calls, wasted resources, annoying the user with irrelevant tool actions), the `shawhin/gemma-3-1b-tool-use` model is superior due to its perfect Precision.\n",
    "*   If **False Negatives are more costly** (e.g., failing to answer a question that requires a tool, missing critical actions), the `google/gemma-3-1b-it` model has a slight edge in Recall, missing one less case.\n",
    "\n",
    "Given the higher F1-score and perfect Precision, `shawhin/gemma-3-1b-tool-use` appears to be the better model for *deciding when to trigger a tool*, assuming the slightly higher chance of missing a required tool call is acceptable.\n",
    "\n",
    "\n",
    "What are the actions we can take to increase accuracy, precision, and recall?\n",
    "\n",
    "\n",
    "Okay, let's discuss actions to potentially improve Accuracy, Precision, and Recall for these tool-calling models, based on the type of errors observed (False Positives and False Negatives).\n",
    "\n",
    "**General Strategies:**\n",
    "\n",
    "1.  **More/Better Training Data:**\n",
    "    *   **More Data:** Simply increasing the volume of high-quality training data can help the model learn more robust patterns.\n",
    "    *   **Targeted Data Augmentation:**\n",
    "        *   **To Reduce False Negatives (Improve Recall):** Add more examples similar to the ones the model missed (where `expected_tool_name` was not None, but `model_tool_name` was None). This includes more diverse phrasing, edge cases, and subtle cues that indicate a tool is needed.\n",
    "        *   **To Reduce False Positives (Improve Precision):** Add more \"hard negative\" examples. These are queries that *seem* like they might need a tool but actually don't. This helps the model learn the boundaries better. For instance, questions *about* tools vs. questions *requiring* tools.\n",
    "    *   **Data Quality:** Ensure labels (`expected_tool_name`) are accurate and consistent. Noise in labels can confuse the model.\n",
    "\n",
    "2.  **Model Architecture/Size:**\n",
    "    *   Experiment with larger versions of the Gemma models if available and computationally feasible. Larger models often have a greater capacity to learn nuances.\n",
    "    *   If fine-tuning, explore different base models.\n",
    "\n",
    "3.  **Fine-tuning Strategies:**\n",
    "    *   **Hyperparameter Tuning:** Adjust learning rate, batch size, number of epochs, weight decay, etc.\n",
    "    *   **Loss Function:** While standard cross-entropy is common, explore techniques like focal loss if there's a class imbalance issue (though the TN count suggests \"NO_TOOL\" isn't rare).\n",
    "    *   **Curriculum Learning:** Start with easier examples and gradually introduce harder ones.\n",
    "\n",
    "4.  **Prompt Engineering (If applicable):**\n",
    "    *   If the model call is influenced by a system prompt, refine the prompt to give clearer instructions about when to call tools and when not to. Include examples of true positives, true negatives, false positives, and false negatives in the prompt's few-shot examples.\n",
    "    *   Clearly define the scope of each tool.\n",
    "\n",
    "**Specific Actions Based on Observed Errors:**\n",
    "\n",
    "*   **To Improve `google/gemma-3-1b-it`'s Precision (Reduce its 3 FPs):**\n",
    "    *   **Analyze FP Examples:** Look at the 3 queries where it called a tool but shouldn't have. What makes them tricky?\n",
    "    *   **Add Hard Negatives:** Create training examples that resemble these FP cases but are correctly labeled as 'NO_TOOL'. This teaches the model to be more discerning.\n",
    "    *   **Refine Tool Descriptions:** Ensure the model understands the limitations of each tool, so it doesn't try to apply them too broadly.\n",
    "\n",
    "*   **To Improve `shawhin/gemma-3-1b-tool-use`'s Recall (Reduce its 4 FNs):**\n",
    "    *   **Analyze FN Examples:** Examine the 4 queries where a tool was needed but not called. Why did the model miss them? Was the intent unclear? Was it a rare tool?\n",
    "    *   **Augment TP Examples:** Add more training data similar to these missed cases, potentially with more explicit phrasing indicating the need for the specific tool.\n",
    "    *   **Improve Cue Recognition:** Fine-tune the model to be more sensitive to phrases or contexts that signal the need for a tool.\n",
    "\n",
    "*   **To Improve `google/gemma-3-1b-it`'s Recall (Reduce its 3 FNs):**\n",
    "    *   Similar to the above, analyze the 3 missed cases and enrich the training data with similar examples requiring tool calls.\n",
    "\n",
    "**Advanced Techniques:**\n",
    "\n",
    "1.  **Confidence Thresholding:** If the model outputs a confidence score for tool prediction, you could adjust the threshold.\n",
    "    *   *Increasing* the threshold makes the model more conservative (fewer FPs, potentially more FNs) -> Increases Precision, might decrease Recall.\n",
    "    *   *Decreasing* the threshold makes it more aggressive (more FPs, potentially fewer FNs) -> Decreases Precision, might increase Recall.\n",
    "    (This is more applicable if the output isn't just the tool name but also a likelihood).\n",
    "\n",
    "2.  **Reinforcement Learning from Human Feedback (RLHF):** Collect human judgments on the model's tool calls and use RL to fine-tune the model to better align with desired behavior, balancing precision and recall.\n",
    "\n",
    "3.  **Error Analysis Loop:** Continuously:\n",
    "    *   Evaluate the model.\n",
    "    *   Analyze the errors (FPs and FNs).\n",
    "    *   Collect or generate new data targeting these errors.\n",
    "    *   Retrain/fine-tune.\n",
    "    *   Repeat.\n",
    "\n",
    "By systematically analyzing the types of mistakes each model makes and enriching the training data accordingly, you can often guide the models toward better performance on your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a0087b1-779c-4f4d-90df-8cb2a148d7b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_model_performance_by_column(results_df, split_column):\n",
    "    \"\"\"\n",
    "    Compare model performance based on tool calling metrics, split by specified column.\n",
    "    \n",
    "    Parameters:\n",
    "    results_df (pd.DataFrame): DataFrame containing evaluation results with columns:\n",
    "        - model_name\n",
    "        - model_called_tool_when_needed\n",
    "        - model_called_correct_tool\n",
    "        - tool_call_success\n",
    "        - expected_tool_name\n",
    "        - split_column (the column to split analysis by)\n",
    "    split_column (str): Column name to split the analysis by (e.g., 'query_type', 'num_tools_available')\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Performance comparison with models and split_column values as rows and metrics as columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter for rows where a tool call was needed (expected_tool_name is not None/NaN)\n",
    "    tool_needed_rows = results_df[results_df['expected_tool_name'].notna() & \n",
    "                                 (results_df['expected_tool_name'] != 'nan')]\n",
    "    \n",
    "    # Group by model name and split column, calculate pass rates for all rows\n",
    "    performance_metrics = results_df.groupby(['model_name', split_column]).agg({\n",
    "        'model_called_tool_when_needed': 'mean',\n",
    "    }).round(4)\n",
    "    \n",
    "    # Calculate metrics only for rows where tool was needed\n",
    "    if len(tool_needed_rows) > 0:\n",
    "        tool_metrics = tool_needed_rows.groupby(['model_name', split_column]).agg({\n",
    "            'model_called_correct_tool': 'mean',\n",
    "            'tool_call_success': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Combine the metrics\n",
    "        performance_metrics = pd.concat([performance_metrics, tool_metrics], axis=1)\n",
    "    else:\n",
    "        # If no rows need tools, set these metrics to None\n",
    "        performance_metrics['model_called_correct_tool'] = None\n",
    "        performance_metrics['tool_call_success'] = None\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    performance_metrics.columns = [\n",
    "        'Tool Called When Needed (%)',\n",
    "        'Correct Tool Called (%)', \n",
    "        'Tool Call Success (%)'\n",
    "    ]\n",
    "    \n",
    "    # Convert to percentages\n",
    "    performance_metrics = performance_metrics * 100\n",
    "    \n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e2c56-8c9f-48a1-be54-084bd8417d6f",
   "metadata": {},
   "source": [
    "#### global results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d067d6a-f69b-47f5-9b35-e668b1ec31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_summary = compare_model_performance(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "617315a5-49ad-48b3-b8f8-4f309d8d42e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool Called When Needed (%)</th>\n",
       "      <th>Correct Tool Called (%)</th>\n",
       "      <th>Tool Call Success (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>google/gemma-3-1b-it</th>\n",
       "      <td>90.00</td>\n",
       "      <td>35.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shawhin/gemma-3-1b-tool-use</th>\n",
       "      <td>93.33</td>\n",
       "      <td>52.5</td>\n",
       "      <td>67.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Tool Called When Needed (%)  \\\n",
       "model_name                                                 \n",
       "google/gemma-3-1b-it                               90.00   \n",
       "shawhin/gemma-3-1b-tool-use                        93.33   \n",
       "\n",
       "                             Correct Tool Called (%)  Tool Call Success (%)  \n",
       "model_name                                                                   \n",
       "google/gemma-3-1b-it                            35.0                   50.0  \n",
       "shawhin/gemma-3-1b-tool-use                     52.5                   67.5  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601d8c83-46df-4e57-8504-98c22eb6e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to file\n",
    "eval_summary.to_csv('data/eval_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb6687-2157-47a5-b722-ff44fb50fe6d",
   "metadata": {},
   "source": [
    "#### results by query type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f2ba25d-8ea2-44a1-abb8-3e4d3cc8c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_summary_by_type = compare_model_performance_by_column(results_df, 'query_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1776088f-530e-4d61-aa3d-a704646231a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tool Called When Needed (%)</th>\n",
       "      <th>Correct Tool Called (%)</th>\n",
       "      <th>Tool Call Success (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th>query_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">google/gemma-3-1b-it</th>\n",
       "      <th>easy</th>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_tool</th>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">shawhin/gemma-3-1b-tool-use</th>\n",
       "      <th>easy</th>\n",
       "      <td>95.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>85.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_tool</th>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Tool Called When Needed (%)  \\\n",
       "model_name                  query_type                                \n",
       "google/gemma-3-1b-it        easy                              100.0   \n",
       "                            hard                              100.0   \n",
       "                            no_tool                            70.0   \n",
       "shawhin/gemma-3-1b-tool-use easy                               95.0   \n",
       "                            hard                               85.0   \n",
       "                            no_tool                           100.0   \n",
       "\n",
       "                                        Correct Tool Called (%)  \\\n",
       "model_name                  query_type                            \n",
       "google/gemma-3-1b-it        easy                           50.0   \n",
       "                            hard                           20.0   \n",
       "                            no_tool                         NaN   \n",
       "shawhin/gemma-3-1b-tool-use easy                           60.0   \n",
       "                            hard                           45.0   \n",
       "                            no_tool                         NaN   \n",
       "\n",
       "                                        Tool Call Success (%)  \n",
       "model_name                  query_type                         \n",
       "google/gemma-3-1b-it        easy                         65.0  \n",
       "                            hard                         35.0  \n",
       "                            no_tool                       NaN  \n",
       "shawhin/gemma-3-1b-tool-use easy                         80.0  \n",
       "                            hard                         55.0  \n",
       "                            no_tool                       NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_summary_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2cdf8-de7f-428a-8704-20949fb585ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tool-use-ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
